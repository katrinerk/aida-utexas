"""
Author: Katrin Erk, Jul 2019
- Simple post-hoc filter for hypothesis files

Update: Pengxiang Cheng, May 2020
- Re-writing for dockerization

Update: Pengxiang Cheng, Aug 2020
- Use the new JsonGraph API
"""

import json
import logging
import sys
from argparse import ArgumentParser
from collections import defaultdict


from aida_utexas import util
from aida_utexas.aif import JsonGraph
from aida_utexas.hypothesis import AidaHypothesisCollection, AidaHypothesisFilter


# Katrin Erk Oct 2020:
# remove hypotheses that have too few events overall.
# for now, set to require at least two events per hypothesis.
# also, not just a place role and nothing else
def hypothesis_too_short(hypothesis, json_graph):
    eretypes = [str(hypothesis.node_type(e)) for e in hypothesis.eres()]
    # if eretypes.count("Event") < 2:
    #     logging.info("KATRIN FILTERED AWAY HYPOTHESIS")
    if eretypes.count("Event") < 2:
        return True

    roles = set()
    for ere in hypothesis.core_eres():
        if json_graph.is_event(ere) or json_graph.is_relation(ere):
            for stmt, pred, obj in hypothesis.event_rel_each_arg_stmt(ere):
                
                # keep this if the obj is an entity
                if json_graph.is_entity(obj):
                    roles.add(pred)
    if len(roles) == 1 and all(r.endswith("Place") for r in roles):
        return True

    return False
    
# Katrin Erk Jn 2021
# remove duplicates in hypotheses
###########
# make a dictionary
# Event/rel name .role name -> set(filler ID)
# for core events and relations (relations that contain at least one statement
# that the SoIN asks about)
def make_core_rolefiller_dict_forhyp(hyp, json_graph):
    retv = { }
    
    for ere in hyp.core_eres():
        if json_graph.is_event(ere) or json_graph.is_relation(ere):
            for stmt, pred, obj in hyp.event_rel_each_arg_stmt(ere):
                
                # keep this if the obj is an entity
                if json_graph.is_entity(obj):
                    if pred not in retv:
                        retv[ pred] = set()
                        
                    # store pred  -> entity
                    retv[pred].add(obj)
    return retv

# compares two dictionaries as generated by
# make_core_rolefiller_dict_forhyp
# return True if the 1st is a true sub-structure of the 2nd
def is_smallerthan(profile1, profile2):
    if all(r in profile2 for r in profile1) and all(profile1[r].issubset(profile2[r]) for r in profile1):
        return True
    else:
        return False
    

def compactify(hypotheses, json_graph):
    # make a profile of core role fillers for each hypothesis
    hyp_profile = { }
    for idx, hyp in enumerate(hypotheses):
        hyp_profile[idx] = make_core_rolefiller_dict_forhyp(hyp, json_graph)

    # determine exact duplicates and remove them
    smallerthan = defaultdict(set)
    toeliminate = set()
    
    for idx1 in range(len(hyp_profile) - 1):
        for idx2 in range(idx1 + 1, len(hyp_profile)):
            if is_smallerthan(hyp_profile[idx2], hyp_profile[idx1]):
                smallerthan[idx1].add(idx2)
                toeliminate.add(idx2)
            elif is_smallerthan(hyp_profile[idx1], hyp_profile[idx2]):
                smallerthan[idx2].add(idx1)
                toeliminate.add(idx1)

    if len(toeliminate) > 0:
        logging.info(f'Duplicate removal: Eliminating {len(toeliminate)} of {len(hypotheses)} hypotheses.')
        
    return list(h for idx, h in enumerate(hypotheses) if idx not in toeliminate)

def main():
    parser = ArgumentParser()

    parser.add_argument('graph_path', help='Path to the input graph JSON file')
    parser.add_argument('hypotheses_path',
                        help='Path to the raw hypotheses file, or a directory with multiple files')
    parser.add_argument('output_dir',
                        help='Directory to write the filtered hypothesis files(s)')
    parser.add_argument('-f', '--force', action='store_true', default=False,
                        help='If specified, overwrite existing output files without warning')

    args = parser.parse_args()

    output_dir = util.get_output_dir(args.output_dir, overwrite_warning=not args.force)

    json_graph = JsonGraph.from_dict(util.read_json_file(args.graph_path, 'JSON graph'))
    hypotheses_file_paths = util.get_file_list(args.hypotheses_path, suffix='.json', sort=True)

    for hypotheses_file_path in hypotheses_file_paths:
        hypotheses_json = util.read_json_file(hypotheses_file_path, 'hypotheses')
        hypothesis_collection = AidaHypothesisCollection.from_json(hypotheses_json, json_graph)

        hypothesis_collection.expand()

        # create the filter
        hypothesis_filter = AidaHypothesisFilter(json_graph)

        filtered_hyplist = [hypothesis_filter.filtered(hypothesis) for hypothesis in hypothesis_collection\
                 if not hypothesis_too_short(hypothesis, json_graph)]

        filtered_hypothesis_collection = AidaHypothesisCollection(compactify(filtered_hyplist, json_graph))

        filtered_hypotheses_json = filtered_hypothesis_collection.to_json()

        # add graph filename and queries, if they were there before
        if 'graph' in hypotheses_json:
            filtered_hypotheses_json['graph'] = hypotheses_json['graph']
        if "queries" in hypotheses_json:
            filtered_hypotheses_json['queries'] = hypotheses_json['queries']

        output_path = output_dir / hypotheses_file_path.name
        logging.info('Writing filtered hypotheses to {} ...'.format(output_path))

        with open(str(output_path), 'w') as fout:
            json.dump(filtered_hypotheses_json, fout, indent=1)


if __name__ == '__main__':
    main()

"""
Author: Katrin Erk, Jul 2019
- Simple post-hoc filter for hypothesis files

Update: Pengxiang Cheng, May 2020
- Re-writing for dockerization

Update: Pengxiang Cheng, Aug 2020
- Use the new JsonGraph API
"""
import sys
import json
import logging
import sys
import copy
from argparse import ArgumentParser
from collections import defaultdict


from aida_utexas import util
from aida_utexas.aif import JsonGraph
from aida_utexas.hypothesis import AidaHypothesisCollection, AidaHypothesisFilter


# Katrin Erk Oct 2020:
# remove hypotheses that have too few events overall.
# for now, set to require at least two events per hypothesis.
# also, not just a place role and nothing else
def hypothesis_too_short(hypothesis, json_graph):
    eretypes = [str(hypothesis.node_type(e)) for e in hypothesis.eres()]
    if eretypes.count("Event") < 2:
        return True

    roles = set()
    for ere in hypothesis.core_eres():
        if json_graph.is_event(ere) or json_graph.is_relation(ere):
            for stmt, pred, obj in hypothesis.event_rel_each_arg_stmt(ere):
                
                # keep this if the obj is an entity
                if json_graph.is_entity(obj):
                    roles.add(pred)
    if len(roles) == 1 and all(r.endswith("Place") for r in roles):
        return True

    return False
    
# Katrin Erk Jn 2021
# remove duplicates in hypotheses
###########
# make a dictionary
# Event/rel name .role name -> set(filler ID)
# for core events and relations (relations that contain at least one statement
# that the SoIN asks about)
def make_core_rolefiller_dict_forhyp(hyp, json_graph):
    retv = { }
    
    for ere in hyp.core_eres():
        if json_graph.is_event(ere) or json_graph.is_relation(ere):
            for stmt, pred, obj in hyp.event_rel_each_arg_stmt(ere):
                
                # keep this if the obj is an entity
                if json_graph.is_entity(obj):
                    if pred not in retv:
                        retv[ pred] = set()
                        
                    # store pred  -> entity
                    retv[pred].add(obj)
    return retv

# compares two dictionaries as generated by
# make_core_rolefiller_dict_forhyp
# return True if the 1st is a true sub-structure of the 2nd
def is_smallerthan(profile1, profile2):
    if all(r in profile2 for r in profile1) and all(profile1[r].issubset(profile2[r]) for r in profile1):
        return True
    else:
        return False
    

def compactify(hypotheses, json_graph):
    # make a profile of core role fillers for each hypothesis
    hyp_profile = { }
    for idx, hyp in enumerate(hypotheses):
        hyp_profile[idx] = make_core_rolefiller_dict_forhyp(hyp, json_graph)

    # determine exact duplicates and remove them
    smallerthan = defaultdict(set)
    toeliminate = set()
    
    for idx1 in range(len(hyp_profile) - 1):
        for idx2 in range(idx1 + 1, len(hyp_profile)):
            if is_smallerthan(hyp_profile[idx2], hyp_profile[idx1]):
                smallerthan[idx1].add(idx2)
                toeliminate.add(idx2)
            elif is_smallerthan(hyp_profile[idx1], hyp_profile[idx2]):
                smallerthan[idx2].add(idx1)
                toeliminate.add(idx1)

    if len(toeliminate) > 0:
        logging.info(f'Duplicate removal: Eliminating {len(toeliminate)} of {len(hypotheses)} hypotheses.')
        
    return list(h for idx, h in enumerate(hypotheses) if idx not in toeliminate)

def make_core_rolefiller_dict_forhyp_2(hyp, json_graph):
    retv = {}

    for qvar, filler in hyp.qvar_filler.items():
        # don't report on events and relations, just entities
        if json_graph.is_event(filler) or json_graph.is_relation(filler):
            continue

        # also don't report on entry points
        if qvar in hyp.qvar_entrypoints:
            continue
        
        ent_dict = hyp.entity_characterization(filler)
        retv[qvar] = { "names" : ent_dict["names"]}
        if "otherinfo" in ent_dict:
            retv[qvar]["otherinfo"] = "#".join(sorted(ent_dict["otherinfo"]))
        else:
            retv[qvar]["otherinfo"] = None

    return retv

#####3
# test whether hypothesis "lower" is subsumed by "upper"
def hyp_subsumed(idxlower, idxupper, lower, upper):
    # all query variables appearing in lower must also be in upper
    if any(qvar not in upper for qvar in lower):
        # print(idxlower, "not subsumed by", idxupper, "qvar mismatch:", ", ".join([q for q in lower if q not in upper]))
        return False

    # for all query variables in lower, we must have a matching name and
    # otherinfo in upper
    for qvar in lower:
        if not(is_same_name(set(lower[qvar]["names"]), set(upper[qvar]["names"]))):
            # print(idxlower, "not subsumed by", idxupper, "name mismatch:", lower[qvar]["names"][:2], upper[qvar]["names"][:2])
            return False
        # for now, don't look at other info when computing subsumption
        # if lower[qvar]["otherinfo"] is not None and lower[qvar]["otherinfo"] != upper[qvar]["otherinfo"]:
        #     print(idxlower, "not subsumed by", idxupper, "otherinfo mismatch:", "\n\t", lower[qvar]["otherinfo"], "\n\t", upper[qvar]["otherinfo"])
        #     return False

    return True
    
def compactify2(hypotheses, json_graph):
    hyp_profile = { }
    for idx, hyp in enumerate(hypotheses):
        hyp_profile[ idx ] = make_core_rolefiller_dict_forhyp_2(hyp, json_graph)
 
    # remove hypotheses whose core fillers are a subset of a previous one
    toeliminate = set()
    
    for idx1 in range(1, len(hyp_profile)):
        for idx2 in range(idx1 - 1):
            if hyp_subsumed(idx1, idx2, hyp_profile[idx1], hyp_profile[idx2]):
                # print("===================")
                # print("Eliminating hypothesis", idx1)
                # print("\tProfile:\n", json.dumps(hyp_profile[idx1], indent = 4))
                # print("\tSubsumed by:\n", json.dumps(hyp_profile[idx2], indent = 4))
                toeliminate.add(idx1)
                break

    if len(toeliminate) > 0:
        logging.info(f'Duplicate removal: Eliminating {len(toeliminate)} of {len(hypotheses)} hypotheses.')
        
    return list(h for idx, h in enumerate(hypotheses) if idx not in toeliminate)
    


# Yejin Cho (September 2021)
# remove redundancies within a single hypothesis
###########
def is_same_name(names1, names2):
    # if two name sets have any shared name, decide as having 'same' names
    if names1.intersection(names2):
        return True
    else:
        return False

def collect_argnames_by_labels(er):
    argnames_by_labels = defaultdict(list)

    for arg in er['arguments']:
        arglabel = arg['arglabel'].split('_')[-1]
        argnames_by_labels[arglabel].append(set(arg['names']))
    return argnames_by_labels


def is_same_verbalization(er1, er2, hypothesis):
    # 1. Events with the exact same verbalization
    # Remove redundant events that have the exact same verbalization, even if they are about different nodes.
    # So, if we have "Oscar attacked" and "Oscar attacked", even if the attack events are different nodes in the graph,
    # we can ditch one of them.
    verb1 = er1['event_relation']['type'].split('.')[-1]
    verb2 = er2['event_relation']['type'].split('.')[-1]
    arglabels1 = [arg['arglabel'].split('_')[-1] for arg in er1['arguments']]
    arglabels2 = [arg['arglabel'].split('_')[-1] for arg in er2['arguments']]

    if verb1 == verb2:
        if set(arglabels1) == set(arglabels2):
            arg_names1 = collect_argnames_by_labels(er1)
            arg_names2 = collect_argnames_by_labels(er2)

            is_samename_record = []
            for lb in arg_names1:
                namesets1 = arg_names1[lb]
                namesets2 = arg_names2[lb]

                namesets1_copy = copy.deepcopy(namesets1)
                for n1 in namesets1:
                    for n2 in namesets2:
                        if is_same_name(n1, n2):
                            try:
                                namesets1_copy.remove(n1)
                            except ValueError:  # this arises when the name is already matched and removed
                                continue
                        else:
                            continue

                namesets2_copy = copy.deepcopy(namesets2)
                for n2 in namesets2:
                    for n1 in namesets1:
                        if is_same_name(n2, n1):
                            try:
                                namesets2_copy.remove(n2)
                            except ValueError:  # this arises when the name is already matched and removed
                                continue
                        else:
                            continue

                if len(namesets1_copy) == 0 and len(namesets2_copy) == 0:
                    is_samename_record.append(True)
                else:
                    is_samename_record.append(False)

            if all(is_samename_record):
                # print('>>> Same')
                # er_printer(er1, hypothesis)
                # er_printer(er2, hypothesis)
                return True

            else:
                # print('>>> Not same')
                # er_printer(er1, hypothesis)
                # er_printer(er2, hypothesis)
                return False

    # if not judged as having the same verbalization
    return False

def er_printer(er, hypothesis):
    arg_info = defaultdict(dict)
    arg_info['type'] = er['event_relation']['type'].split('.')[-1]
    for m, x in enumerate(er['arguments']):
        key = x['arglabel'].split('_')[-1]
        value = hypothesis.entity_best_name(x['node'])
        cnt = 0
        if arg_info[key + str(cnt)]:
            cnt += 1
        arg_info[key + str(cnt)] = value
    print(json.dumps(arg_info, sort_keys=True, indent=4))
    return True

# test if er1's verbalization is a subset of that of er2, and the other way round.
# returns:
#  0 if no inclusion either way.
#  1 if er1 is included in er2
#  -1 if er2 is included in er1

def test_subset_verbalization(er1, er2):
    #extract event types
    verb1 = er1['event_relation']['type'].split('.')[-1]
    verb2 = er2['event_relation']['type'].split('.')[-1]

    if verb1 != verb2:
        # not the same verbalization of the verb, so this won't be a subset case
        return 0

    # make a set that characterizes the verbalization of each argument set
    arg1set = _arg_characterization_as_set(er1)
    arg2set = _arg_characterization_as_set(er2)

    if arg1set.issubset(arg2set):
        return 1
    elif arg2set.issubset(arg1set):
        return -1
    else:
        return 0

def _arg_characterization_as_set(er):
    retv = set()

    for arg in er["arguments"]:
        piece1 = arg["arglabel"]
        piece2 = "@".join(sorted(arg["names"]))
        if "otherinfo" in arg:
            piece3 = "#".join(sorted(arg["otherinfo"]))
        else:
            pieces3 = ""
            
        retv.add("|".join([piece1, piece2, piece3]))

    return retv
    
def has_uninformative_entities(er):
    # 2. Uninformative entities
    # Entities that are called only "he" or "she" or "something", AND with no additional information attached.
    # (We ignore pronouns in other languages for now.)
    # What "additional information attached" means:
    #   a node that is only called "he", but for which we have additional types and events, like:
    #   this "he" is a government official, and this "he" was involved in protests earlier.
    args = er['arguments']
    uninformative_pronouns = ['he', 'she', 'something', 'him', 'her', 'his', 'hers', 'himself', 'herself',
                              'it', 'they', 'them', 'their', 'themselves', 'itself', 'its', 'someone', 'we', 'our',
                              'who', 'which', 'what', 'whom', 'where', 'how', 'some person', 'some people',
                              'I', 'my', 'you', 'your', 'yours', 'mine']
    for arg in args:
        if len(arg['names']) == 0:  # no name at all
            return True, arg['node']
        elif len(arg['names']) == 1 and arg['names'][0].isspace():  # consider whitespace as an uninformative name
            return True, arg['node']
        elif len(arg['otherinfo']) == 0:  # no additional information attached
            arg_names = [name.lower() for name in arg['names']]
            if all((n in uninformative_pronouns or n == "") for n in arg_names):  # and called only "he" or "she" or "something"
                return True, arg['node']

    # if not judged as having uninformative entities
    return False, None

def has_uninformative_events(er):
    # 3. Uninformative events
    # Some events are unhelpful when they only have a single participant, such as: "Oscar communicated" or "Oscar moved".
    # Other events are helpful even when they only have a single participant, such as 'Oscar died".
    # We'd need a list of events that are uninformative when they only have one participant
    # (as a first attempt: "communicate" and "move"). We can ditch uninformative events.
    verb = er['event_relation']['type'].split('.')[-1].lower()
    less_informative_verbs = ["communicate", "move"]
    if verb in less_informative_verbs and len(er['arguments']) == 1:
        return True


def compactify_within_hypothesis(hypothesis):
    # counters for each redundancy type
    cnts_type = [0, 0, 0]

    # characterize all the ERs in a given hypothesis
    eventrelations_list = list(hypothesis.each_eventrelation_characterization())

    # determine redundancies and remove them
    toeliminate = set()

    # downrank (instead of eliminate) when judged as only having some uninformative "core" entities
    todownrank = False

    # [type 1] redundant events with identical verbalizations
    for idx1 in range(len(eventrelations_list) - 1):
        for idx2 in range(idx1 + 1, len(eventrelations_list)):
            er1 = eventrelations_list[idx1]
            er2 = eventrelations_list[idx2]
            if is_same_verbalization(er1, er2, hypothesis):
                toeliminate.add(idx2)  # remove the second event/relation in comparison
                cnts_type[0] += 1

    # also type 1: subset verbalization
    for idx1 in range(len(eventrelations_list) - 1):
        for idx2 in range(idx1 + 1, len(eventrelations_list)):
            classif = test_subset_verbalization(eventrelations_list[idx1], eventrelations_list[idx2])
            if classif == -1:
                # ER2 is subset of ER1
                toeliminate.add(idx2)
                cnts_type[0] += 1
            elif classif == 1:
                # ER1 is subset of ER2
                toeliminate.add(idx1)
                cnts_type[0] += 1
                # we just lost ER1, so we can skip the rest of the ER2's
                break

    # [type 2] uninformative entities (too generic; called only "he", "she", or "something")
    # retrieve core EREs first
    core_eres = hypothesis.core_eres()
    for idx in range(len(eventrelations_list)):
        tf, entity_node = has_uninformative_entities(eventrelations_list[idx])
        if tf and entity_node:
            # If an uninformative entity is not "core" (not one of the query entities that the Statement of
            # Information Need asked about), we ditch it. If it is "core", we downrank the hypothesis.
            if entity_node in core_eres:
                todownrank = True
            else:
                toeliminate.add(idx)
                cnts_type[1] += 1

    # [type 3] uninformative events
    # (as a first attempt: when a "communicate" or "move" event only has a single participant, consider it uninformative.)
    for idx in range(len(eventrelations_list)):
        if has_uninformative_events(eventrelations_list[idx]):
            toeliminate.add(idx)
            cnts_type[2] += 1

    if len(toeliminate) > 0:
        logging.info(f'Redundancy removal within a hypothesis: Eliminating {len(toeliminate)} of {len(eventrelations_list)} events/relations.')
        logging.info(f'Counts per redundancy types: {cnts_type[0]} identical verbalizations, {cnts_type[1]} uninformative entities, {cnts_type[2]} uninformative events')

    for idx in toeliminate:
        stmts_to_eliminate = eventrelations_list[idx]['statements']
        for stmt in stmts_to_eliminate:
            try:
                hypothesis.stmts.remove(stmt)
                hypothesis.stmt_weights.pop(stmt)
            except (KeyError, ValueError) as e:  # this arises when the stmt was previously removed
                continue

    return hypothesis, todownrank


def main():
    parser = ArgumentParser()

    parser.add_argument('graph_path', help='Path to the input graph JSON file')
    parser.add_argument('hypotheses_path',
                        help='Path to the raw hypotheses file, or a directory with multiple files')
    parser.add_argument('output_dir',
                        help='Directory to write the filtered hypothesis files(s)')
    parser.add_argument('-f', '--force', action='store_true', default=False,
                        help='If specified, overwrite existing output files without warning')

    args = parser.parse_args()

    output_dir = util.get_output_dir(args.output_dir, overwrite_warning=not args.force)

    json_graph = JsonGraph.from_dict(util.read_json_file(args.graph_path, 'JSON graph'))
    hypotheses_file_paths = util.get_file_list(args.hypotheses_path, suffix='.json', sort=True)


    for hypotheses_file_path in hypotheses_file_paths:
    
        # print(f'\n>>> SoIN #{hypotheses_file_path}')
        hypotheses_json = util.read_json_file(hypotheses_file_path, 'hypotheses')
        hypothesis_collection = AidaHypothesisCollection.from_json(hypotheses_json, json_graph)

        hypothesis_collection.expand()

        # create the filter
        hypothesis_filter = AidaHypothesisFilter(json_graph)

        # filter hypotheses list
        filtered_hyplist = [hypothesis_filter.filtered(hypothesis) for hypothesis in hypothesis_collection\
                 if not hypothesis_too_short(hypothesis, json_graph)]

        # filter again to reduce redundancies within each hypothesis
        double_filtered_hyplist = []

        for hyp_i, h in enumerate(filtered_hyplist):
            logging.info(f'---NEW HYPO #{hyp_i}--')
            filtered_h, downrank = compactify_within_hypothesis(h)
            double_filtered_hyplist.append(filtered_h)
            if downrank:
                h.update_weight(-1)

        filtered_hypothesis_collection = AidaHypothesisCollection(compactify2(double_filtered_hyplist, json_graph))

        filtered_hypotheses_json = filtered_hypothesis_collection.to_json()

        # add graph filename and queries, if they were there before
        if 'graph' in hypotheses_json:
            filtered_hypotheses_json['graph'] = hypotheses_json['graph']
        if "queries" in hypotheses_json:
            filtered_hypotheses_json['queries'] = hypotheses_json['queries']

        output_path = output_dir / hypotheses_file_path.name
        logging.info('Writing filtered hypotheses to {} ...'.format(output_path))

        with open(str(output_path), 'w') as fout:
            json.dump(filtered_hypotheses_json, fout, indent=1)


if __name__ == '__main__':
    main()
